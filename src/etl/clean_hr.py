# src/etl/clean_hr.py
# -*- coding: utf-8 -*-
"""
Limpieza y normalizaci√≥n del dataset HR_Data_MNC_Data Science Lovers
=====================================================================

QU√â HACE
--------
1) Estandariza columnas a un esquema can√≥nico en ingl√©s (interno del script).
2) Limpia/normaliza: tipos, dominios (overtime), nulos, duplicados, outliers (winsor 1%-99%).
3) Renombra las columnas can√≥nicas al **espa√±ol** para una mejor comprenci√≥n.
4) Valida reglas cr√≠ticas (unicidad y rompe si fallan .
5) Guarda el resultado en Parquet o CSV .
6) Escribe una auditor√≠a m√≠nima (filas in/out y columnas).
"""

from __future__ import annotations

import json
import logging
import time
from pathlib import Path
from typing import Tuple, Dict, Any, List

import pandas as pd
from src.config import load_settings  # lee .env + configs/*.yaml

# Logger b√°sico si no tienes uno central
try:
    from src.logging import get_logger
except Exception:
    def get_logger(name: str = "clean_hr"):
        logging.basicConfig(level=logging.INFO, format="%(levelname)s %(message)s")
        return logging.getLogger(name)

logger = get_logger("clean_hr")

# --- Mapeos de nombres (entrada variada ‚Üí can√≥nico EN ‚Üí salida ES) -----------------

# 1) Versiones ‚Äúcomo vengan‚Äù ‚Üí nombres can√≥nicos en ingl√©s (interno)
TO_CANONICAL_EN: Dict[str, str] = {
    # variantes comunes ‚Üí can√≥nico
    "employee_id": "employee_id",
    "employeeid": "employee_id",
    "Employee_ID": "employee_id",
    "Full_Name": "full_name",
    "Department": "department",
    "Job_Title": "job_title",
    "Hire_Date": "hire_date",
    "Location": "location",
    "Performance_Rating": "performance_rating",
    "Experience_Years": "experience_years",
    "Status": "status",
    "Work_Mode": "work_mode",
    "Salary_INR": "salary_inr",
}

# 2) Can√≥nico EN ‚Üí nombre final ES (lo que ver√° BI/PowerBI/usuarios)
EN_TO_ES: Dict[str, str] = {
    "employee_id": "id_empleado",
    "full_name": "nombre_completo",
    "department": "departamento",
    "job_title": "puesto",
    "hire_date": "fecha_contratacion",
    "location": "ubicacion",
    "performance_rating": "calificacion_desempeno",
    "experience_years": "anios_experiencia",
    "status": "estatus",
    "work_mode": "modalidad_trabajo",
    "salary_inr": "salario_inr",
}

# Inversa para resolver validaciones que vengan en ES
ES_TO_EN = {v: k for k, v in EN_TO_ES.items()}


# ------------------------------- Helpers E/S --------------------------------------

def _read_any(path: Path) -> pd.DataFrame:
    """Lee CSV o Parquet con Pandas (requiere pyarrow/fastparquet para Parquet)."""
    suf = path.suffix.lower()
    if suf == ".parquet":
        return pd.read_parquet(path)
    return pd.read_csv(path)


def _write_parquet_or_csv(df: pd.DataFrame, path: Path) -> None:
    """
    Escribe Parquet si es posible; si falla (sin pyarrow), escribe CSV como fallback.
    """
    path.parent.mkdir(parents=True, exist_ok=True)
    try:
        if path.suffix.lower() != ".parquet":
            path = path.with_suffix(".parquet")
        df.to_parquet(path, index=False)  # necesita pyarrow/fastparquet
        logger.info(f"‚úî Guardado Parquet: {path}")
    except Exception as e:
        csv_path = path.with_suffix(".csv")
        df.to_csv(csv_path, index=False)
        logger.warning(f"‚ö† Parquet no disponible ({e}). Se guard√≥ CSV: {csv_path}")


def _to_lower_snake(name: str) -> str:
    """Convierte nombres arbitrarios a snake_case (min√∫sculas + gui√≥n bajo)."""
    return (
        name.strip()
        .replace(" ", "_").replace("-", "_").replace("/", "_")
        .replace("__", "_").lower()
    )


def _winsor_limits(series: pd.Series, q_low: float = 0.01, q_high: float = 0.99) -> Tuple[float, float]:
    """L√≠mites inferior/superior para winsorizar (recortar outliers suaves)."""
    lo = float(series.quantile(q_low))
    hi = float(series.quantile(q_high))
    return lo, hi


def _resolve_col(name: str, cols: List[str]) -> str | None:
    """
    Dada una columna mencionada en reglas (puede venir en EN o ES),
    devuelve el nombre real presente en el DataFrame final.
    """
    if name in cols:
        return name
    # Si vino en EN pero el DF ya est√° en ES
    if name in EN_TO_ES and EN_TO_ES[name] in cols:
        return EN_TO_ES[name]
    # Si vino en ES pero el DF est√° en EN (antes de renombrar)
    if name in ES_TO_EN and ES_TO_EN[name] in cols:
        return ES_TO_EN[name]
    return None


# ----------------------------- Limpieza principal ---------------------------------

def _standardize_to_canonical_en(df: pd.DataFrame) -> pd.DataFrame:
    """
    1) Normaliza nombres a snake_case.
    2) Mapea a nombres can√≥nicos en ingl√©s (interno).
    """
    df = df.copy()
    df.columns = [_to_lower_snake(c) for c in df.columns]

    existing = set(df.columns)
    rename = {c: TO_CANONICAL_EN[c] for c in TO_CANONICAL_EN if c in existing}
    if rename:
        df = df.rename(columns=rename)
    return df


def clean_dataframe(df: pd.DataFrame, rename_to_spanish: bool = True) -> pd.DataFrame:
    """
    Aplica limpieza estable de negocio y (opcional) renombra columnas al espa√±ol.

    Pasos:
      - Est√°ndar de columnas ‚Üí can√≥nico EN.
      - Trim/normalizaci√≥n b√°sica de texto.
      - Dominios: overtime Yes/No ‚Üí 1/0. (attrition idem si viene textual)
      - Casting num√©rico seguro y fechas tolerantes.
      - Valores negativos defensivos (ingreso_mensual) + winsorizaci√≥n 1%-99%.
      - Deduplicaci√≥n (employee_id + snapshot_date si existen).
      - Renombrar a espa√±ol (si rename_to_spanish=True).
    """
    df = _standardize_to_canonical_en(df)

    # 2) Texto: trim y colapsar espacios (heur√≠stica simple por dtype)
    for c in df.columns:
        if pd.api.types.is_object_dtype(df[c]) or pd.api.types.is_string_dtype(df[c]):
            df[c] = df[c].astype("string").str.strip().str.replace(r"\s+", " ", regex=True)

    # 3) Dominios/casting b√°sicos (overtime, attrition)
    if "overtime" in df.columns:
        s = df["overtime"].astype("string").str.lower()
        df["overtime"] = s.isin(["yes", "y", "true", "1", "si", "s√≠"]).astype("int8")

    if "attrition" in df.columns:
        # Si llega textual ("Yes"/"No"), lo llevamos a 0/1
        s = df["attrition"].astype("string").str.lower()
        mask_known = s.isin(["yes", "y", "true", "1", "no", "n", "false", "0", "si", "s√≠"])
        if mask_known.any():
            df.loc[mask_known, "attrition"] = s[mask_known].isin(["yes", "y", "true", "1", "si", "s√≠"]).astype("int8")
        df["attrition"] = pd.to_numeric(df["attrition"], errors="coerce").astype("Float64")

    # 4) Num√©ricos seguros (NO convertir employee_id, es string)
    for c in ["performance_rating", "age", "overtime_hours", "monthly_income"]:
        if c in df.columns:
            df[c] = pd.to_numeric(df[c], errors="coerce")

    # 5) Fechas tolerantes
    for dcol in ("hire_date", "snapshot_date"):
        if dcol in df.columns:
            df[dcol] = pd.to_datetime(df[dcol], errors="coerce").dt.date

    # 6) Defensivo: ingresos negativos ‚Üí NaN; winsor 1%-99% si hay suficiente volumen
    if "monthly_income" in df.columns:
        df.loc[df["monthly_income"] < 0, "monthly_income"] = pd.NA
        if df["monthly_income"].notna().sum() >= 1000:
            lo, hi = _winsor_limits(df["monthly_income"].dropna(), 0.01, 0.99)
            df["monthly_income"] = df["monthly_income"].clip(lower=lo, upper=hi)

    # 7) Deduplicaci√≥n (preferente por clave compuesta)
    if {"employee_id", "snapshot_date"}.issubset(df.columns):
        df = df.drop_duplicates(subset=["employee_id", "snapshot_date"], keep="first")
    else:
        df = df.drop_duplicates(keep="first")

    # 8) Renombrar al espa√±ol para el output final (si se pide)
    if rename_to_spanish:
        cols = df.columns.tolist()
        # Renombramos solo las que est√©n mapeadas EN‚ÜíES
        rename_es = {en: EN_TO_ES[en] for en in EN_TO_ES if en in cols}
        df = df.rename(columns=rename_es)

    return df


# ---------------------------- Validaciones cr√≠ticas --------------------------------

def quick_validate(df: pd.DataFrame, etl_cfg: Any) -> None:
    """
    Reglas cr√≠ticas desde YAML (pueden venir en ES o EN).
    Rompe el pipeline con ValueError si falla.
    """
    crit: Dict[str, Any] = getattr(etl_cfg, "quality_critical", {}) or {}
    errors: List[str] = []

    # Unicidad
    for name in (crit.get("unique") or []):
        col = _resolve_col(name, df.columns.tolist())
        if col and col in df.columns:
            uniques = int(df[col].nunique(dropna=True))
            total = int(len(df))
            if uniques != total:
                errors.append(f"Unicidad violada en {col}: √∫nicos={uniques}, filas={total}")

    # No-negativos
    for name in (crit.get("non_negative") or []):
        col = _resolve_col(name, df.columns.tolist())
        if col and col in df.columns:
            bad = int((df[col] < 0).sum(skipna=True))
            if bad > 0:
                errors.append(f"Valores negativos en {col}: {bad}")

    # Dominios categ√≥ricos/num√©ricos
    for name, allowed in (crit.get("domain") or {}).items():
        col = _resolve_col(name, df.columns.tolist())
        if col and col in df.columns:
            bad = int((~df[col].isin(allowed)).sum(skipna=True))
            if bad > 0:
                errors.append(f"Valores fuera de dominio en {col}: {bad} (permitidos={allowed})")

    if errors:
        for e in errors:
            logger.error(e)
        raise ValueError(" Validaciones cr√≠ticas fallaron. Revisa reglas y datos.")


# ------------------------------- Auditor√≠a simple ----------------------------------

def audit_stats(df_before: pd.DataFrame, df_after: pd.DataFrame, out_path: Path) -> None:
    """Guarda filas in/out y columnas (trazabilidad m√≠nima)."""
    stats = {
        "rows_in": int(len(df_before)),
        "rows_out": int(len(df_after)),
        "columns": list(map(str, df_after.columns)),
        "engine": "pandas",
    }
    out_path.parent.mkdir(parents=True, exist_ok=True)
    with open(out_path, "w", encoding="utf-8") as f:
        json.dump(stats, f, indent=2)


# ---------------------------------- Main runner ------------------------------------

def main() -> None:
    """
    Orquesta: lee config ‚Üí elige entrada ‚Üí limpia ‚Üí valida ‚Üí guarda ‚Üí audita.
    - Si en tu YAML pusiste `rename_to_spanish: false`, el output se queda en EN can√≥nico.
    """
    app_cfg, etl_cfg, _ = load_settings()

    in_path = Path(getattr(etl_cfg, "interim_path", None) or etl_cfg.raw_path)
    out_path = Path(etl_cfg.processed_path)
    audit_out = Path("artifacts/metrics/clean_stats.json")
    rename_to_spanish = bool(getattr(etl_cfg, "rename_to_spanish", True))

    t0 = time.time()
    logger.info(f"üîπ Leyendo datos de: {in_path}")
    df0 = _read_any(in_path)

    logger.info("üîπ Limpiando/normalizando‚Ä¶")
    df = clean_dataframe(df0, rename_to_spanish=rename_to_spanish)

    logger.info("üîπ Validaciones r√°pidas‚Ä¶")
    quick_validate(df, etl_cfg)

    logger.info(f"üîπ Guardando procesado en: {out_path}")
    _write_parquet_or_csv(df, out_path)

    audit_stats(df0, df, audit_out)
    logger.info(f" Listo en {time.time() - t0:0.2f}s ‚Üí {out_path}")


if __name__ == "__main__":
    main()
