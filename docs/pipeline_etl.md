# Pipeline ETL - TalentPulse

## Resumen

El pipeline ETL de TalentPulse procesa datos de recursos humanos de forma automatizada, incluyendo limpieza, validaciÃ³n de calidad, construcciÃ³n de features y exportaciÃ³n.

## ğŸš€ EjecuciÃ³n RÃ¡pida

```bash
# Activar entorno virtual
.venv\Scripts\activate

# Ejecutar pipeline completo
python -m src.etl.pipeline
```

## ğŸ“‚ Estructura de Archivos

### Archivos ETL Principales
- `src/etl/clean_hr.py` - Limpieza y normalizaciÃ³n de datos
- `src/etl/pipeline.py` - Orquestador principal del pipeline
- `src/etl/run_quality_checks.py` - Validaciones de calidad
- `src/etl/feature_build.py` - ConstrucciÃ³n de caracterÃ­sticas
- `src/etl/export_csv_warehouse.py` - ExportaciÃ³n a warehouse
- `src/etl/to_parquet.py` - ConversiÃ³n a formato Parquet

### Archivos de ConfiguraciÃ³n
- `configs/params_etl.yaml` - ConfiguraciÃ³n de limpieza
- `configs/params_pipeline.yaml` - ConfiguraciÃ³n del pipeline

### Datos Generados
- `Data/processed/clean_hr.csv` - Datos limpios
- `Data/processed/hr_with_features.csv` - Datos con features
- `Data/warehouse/hr_final.csv` - Datos finales para warehouse
- `artifacts/metrics/quality_report.json` - Reporte de calidad

## ğŸ”§ Componentes del Pipeline

### 1. Limpieza de Datos (`clean_hr.py`)
- EstandarizaciÃ³n de nombres de columnas
- ConversiÃ³n de tipos de datos
- NormalizaciÃ³n de valores categÃ³ricos
- DetecciÃ³n y correcciÃ³n de outliers
- EliminaciÃ³n de duplicados
- Validaciones crÃ­ticas de calidad

### 2. ValidaciÃ³n de Calidad (`run_quality_checks.py`)
- AnÃ¡lisis de valores nulos
- DetecciÃ³n de duplicados
- IdentificaciÃ³n de outliers (mÃ©todo IQR)
- AnÃ¡lisis de unicidad
- EstadÃ­sticas por tipo de variable
- GeneraciÃ³n de reporte JSON

### 3. ConstrucciÃ³n de Features (`feature_build.py`)

#### Features de AntigÃ¼edad
- `antiguedad_dias` - DÃ­as desde contrataciÃ³n
- `antiguedad_anos` - AÃ±os desde contrataciÃ³n
- `categoria_antiguedad` - Nuevo/Junior/Intermedio/Senior/Veterano
- `ano_contratacion` - AÃ±o de contrataciÃ³n
- `mes_contratacion` - Mes de contrataciÃ³n

#### Features de Salario
- `salario_percentil` - Percentil salarial (0-100)
- `categoria_salario` - Bajo/Medio-Bajo/Medio-Alto/Alto
- `salario_normalizado` - Z-score del salario

#### Features de DesempeÃ±o
- `categoria_desempeno` - Bajo/Regular/Bueno/Excelente
- `alto_desempeno` - Flag binario (top 20%)

#### Features DemogrÃ¡ficas
- `categoria_experiencia` - Entry/Junior/Mid/Senior/Expert

### 4. ExportaciÃ³n (`export_csv_warehouse.py`)
- ExportaciÃ³n a warehouse con timestamp
- GeneraciÃ³n de archivo principal (hr_final.csv)
- Backup histÃ³rico con timestamp
- Reporte resumen en texto plano

## ğŸ“Š Archivos de Salida

### Datos Procesados
```
Data/
â”œâ”€â”€ processed/
â”‚   â”œâ”€â”€ clean_hr.csv                    # Datos limpios (11 columnas)
â”‚   â”œâ”€â”€ hr_with_features.csv            # Con features (23 columnas)
â”‚   â””â”€â”€ hr_with_features.parquet        # VersiÃ³n Parquet
â””â”€â”€ warehouse/
    â”œâ”€â”€ hr_final.csv                    # Datos finales
    â””â”€â”€ hr_final_20250906_161028.csv    # Backup con timestamp
```

### Reportes y MÃ©tricas
```
artifacts/
â”œâ”€â”€ metrics/
â”‚   â”œâ”€â”€ clean_stats.json               # EstadÃ­sticas de limpieza
â”‚   â””â”€â”€ quality_report.json            # Reporte de calidad completo
â””â”€â”€ reports/
    â””â”€â”€ summary_report.txt              # Resumen ejecutivo
```

## ğŸ” Validaciones de Calidad

### Reglas CrÃ­ticas
- **Unicidad**: `id_empleado` debe ser Ãºnico
- **No nulos**: Columnas crÃ­ticas no pueden tener valores nulos
- **Dominios**: Valores categÃ³ricos dentro de rangos permitidos
- **Outliers**: DetecciÃ³n usando mÃ©todo IQR (rango intercuartil)

### Umbrales de Calidad
- MÃ¡ximo 5% de valores nulos por columna
- MÃ¡ximo 1% de filas duplicadas
- MÃ­nimo 95% de unicidad para IDs

## ğŸ› ï¸ EjecuciÃ³n Individual

### Ejecutar solo limpieza
```bash
python -m src.etl.clean_hr
```

### Ejecutar solo validaciones
```bash
python -m src.etl.run_quality_checks
```

### Ejecutar solo features
```bash
python -m src.etl.feature_build
```

### Ejecutar solo exportaciÃ³n
```bash
python -m src.etl.export_csv_warehouse
```

## ğŸ“‹ Logs y Monitoreo

El pipeline genera logs detallados que incluyen:
- Timestamps de cada paso
- NÃºmero de filas procesadas
- Errores y advertencias
- EstadÃ­sticas de transformaciones
- Tiempos de ejecuciÃ³n

Ejemplo de log tÃ­pico:
```
2025-09-06 16:08:28,719 INFO etl_pipeline: ğŸš€ Iniciando pipeline ETL completo...
2025-09-06 16:08:28,720 INFO etl_pipeline: ğŸ§¹ Iniciando limpieza de datos...
2025-09-06 16:09:09,402 INFO etl_pipeline: âœ… Limpieza completada
2025-09-06 16:10:57,807 INFO etl_pipeline: ğŸ‰ Â¡Pipeline ETL exitoso!
```

## ğŸ”§ ConfiguraciÃ³n

### Variables de Entorno (Opcional)
Crea un archivo `.env` para configuraciones sensibles:
```
DB_HOST=localhost
DB_USER=hr_user
DB_PASSWORD=secure_password
DB_NAME=hr_analytics
```

### PersonalizaciÃ³n
Modifica `configs/params_pipeline.yaml` para:
- Cambiar umbrales de calidad
- Habilitar/deshabilitar features especÃ­ficas
- Configurar formatos de exportaciÃ³n
- Ajustar configuraciÃ³n de logging

## âš¡ Rendimiento

### Tiempos TÃ­picos (2M registros)
- Limpieza: ~40 segundos
- ValidaciÃ³n: ~10 segundos
- Features: ~25 segundos
- ExportaciÃ³n: ~60 segundos
- **Total**: ~2.5 minutos

### Optimizaciones
- Usa formato Parquet para mejor rendimiento
- Ajusta `chunksize` para datasets muy grandes
- Considera paralelizaciÃ³n para mÃºltiples archivos

## ğŸ§ª Testing

Ejecuta las pruebas del pipeline:
```bash
pytest tests/test_etl.py -v
```

## ğŸ“ Soporte

Para problemas o mejoras:
1. Revisa los logs en `artifacts/logs/`
2. Verifica la configuraciÃ³n en `configs/`
3. Consulta este documento
4. Contacta al equipo de Data Engineering
